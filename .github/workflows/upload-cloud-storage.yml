name: Upload Production Masked file to Cloud Storage

on:
  push:
    branches: [ci-cd-playground]
  schedule:
    - cron: "0 2 * * *"
  workflow_dispatch:

env:
  REGISTRY: ghcr.io

jobs:
  upload:
    runs-on: ubuntu-latest
    permissions:
      contents: "read"
      id-token: "write"

    steps:
      - id: "checkout"
        uses: "actions/checkout@v4"

      - name: Install PostgreSQL 18 client tools
        run: |
          # Add the official PostgreSQL repository for version 18
          sudo apt-get update
          sudo apt-get install -y wget gnupg2
          wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -
          echo "deb http://apt.postgresql.org/pub/repos/apt/ $(lsb_release -cs)-pgdg main" | sudo tee /etc/apt/sources.list.d/pgdg.list
          sudo apt-get update

          # Install version 18 specifically
          sudo apt-get install -y postgresql-client-18

      - id: "auth"
        uses: "google-github-actions/auth@v2"
        with:
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}

      - name: "Install Cloud SQL Proxy & Postgres Client"
        run: |
          curl -o cloud-sql-proxy https://storage.googleapis.com/cloud-sql-connectors/cloud-sql-proxy/v2.8.1/cloud-sql-proxy.linux.amd64
          chmod +x cloud-sql-proxy
          sudo apt-get update && sudo apt-get install -y postgresql-client

      - name: "Create database backup"
        run: |
          # Ensure the script fails if pg_dump fails, even with the pipe
          set -euo pipefail

          # Start the proxy in the background
          ./cloud-sql-proxy ${{ secrets.CLOUD_SQL_CONNECTION_NAME }} --port 5432 &
          sleep 5 # Give it a second to initialize

          # Generate a filename with a timestamp
          export BACKUP_NAME="backup-$(date +%Y-%m-%d-%H%M).sql.gz"
          echo "FILENAME=$BACKUP_NAME" >> $GITHUB_ENV

          # Dump, compress on the fly, and save
          pg_dump \
            -h 127.0.0.1 \
            -p 5432 \
            -U ${{ secrets.PROD_MASKED_USER }} \
            -d ${{ secrets.PROD_MASKED_DB }} \
            --no-security-labels \
            --exclude-extension="anon" \
            --no-owner \
            -O | gzip > $BACKUP_NAME
        env:
          PGPASSWORD: ${{ secrets.PROD_MASKED_PASSWORD }}

      - id: "upload-file"
        name: "Upload backup to Google Cloud Storage"
        uses: "google-github-actions/upload-cloud-storage@v2"
        with:
          path: ${{ env.FILENAME }}
          destination: ${{ secrets.GCS_BUCKET_NAME }}

      - name: "Prepare latest pointer"
        run: cp ${{ env.FILENAME }} backup-latest.sql.gz

      - id: "upload-latest"
        name: "Update latest pointer in GCS"
        uses: "google-github-actions/upload-cloud-storage@v2"
        with:
          path: backup-latest.sql.gz
          destination: ${{ secrets.GCS_BUCKET_NAME }}
          parent: false

  local-test-image:
    runs-on: ubuntu-latest
    needs: upload

    steps:
      - uses: actions/checkout@v4
      - name: Log in to GHCR
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - id: "auth"
        uses: google-github-actions/auth@v2
        with:
          workload_identity_provider: ${{ secrets.WIF_PROVIDER }}
          service_account: ${{ secrets.WIF_SERVICE_ACCOUNT }}

      # Set up gcloud CLI
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      - name: Download Sanitized Dump
        run: |
          set -euo pipefail
          gsutil cp gs://${{ secrets.GCS_BUCKET_NAME }}/backup-latest.sql.gz - | gunzip > backup.sql

      - name: Load environment variables
        run: |
          echo "${{ secrets.TEST_ENV_BASE64 }}" | base64 -d > ./app/backend/.env.testing

      - name: Start Database Only
        run: docker compose -f docker-compose.local-test.yml up -d test_postgres_db --wait

        # HYDRATION (Import data into the service container)
      - name: "Hydrate Test Database"
        run: |
          docker compose -f docker-compose.local-test.yml run \
            --rm \
            -v $(pwd)/backup.sql:/backup.sql \
            test_postgres_db \
            sh -c "psql -h test_postgres_db -U ${{ secrets.POSTGRES_USER }} -d ${{ secrets.POSTGRES_DB }} < /backup.sql"

      - name: Build images
        run: docker compose -f docker-compose.local-test.yml build

      # Use 'run' instead of 'exec' before the full stack is up
      - name: Run migrations
        run: docker compose -f docker-compose.local-test.yml run --rm backend alembic upgrade head

      # Now start everything and wait for that healthcheck
      - name: Start Services
        run: docker compose -f docker-compose.local-test.yml up -d --wait
